{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae86fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ea853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag \n",
    "from textblob import TextBlob \n",
    "from wordcloud import WordCloud\n",
    "import numpy as np \n",
    "import re \n",
    "import string \n",
    "import glob \n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39eb2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2573dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcdac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1590057131499307008</td>\n",
       "      <td>2023-02-14 10:27:56+00:00</td>\n",
       "      <td>b'@stqrkkura anemia.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1396131266207100931</td>\n",
       "      <td>2023-02-14 10:27:43+00:00</td>\n",
       "      <td>b'@GRozzana Chama creo que lo mio puede ser an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1431251001198067724</td>\n",
       "      <td>2023-02-14 10:23:57+00:00</td>\n",
       "      <td>b'@jenhoonieee Heheheehehehehehheheeheeee ,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1424407901159006215</td>\n",
       "      <td>2023-02-14 10:20:45+00:00</td>\n",
       "      <td>b'A 51 year old gentleman presented with sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1571175125999030272</td>\n",
       "      <td>2023-02-14 10:20:18+00:00</td>\n",
       "      <td>b\"@tuffimitten @agenetics1 Don't remember the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>284</td>\n",
       "      <td>1196038635658723329</td>\n",
       "      <td>2023-02-14 03:16:29+00:00</td>\n",
       "      <td>b'RT @TorukoInNihon: \\xe6\\x97\\xa5\\xe6\\x9c\\xac\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>285</td>\n",
       "      <td>491441655</td>\n",
       "      <td>2023-02-14 03:16:13+00:00</td>\n",
       "      <td>b\"@jsalamipeter @Your_Miss_B No sinus issues b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>286</td>\n",
       "      <td>191027326</td>\n",
       "      <td>2023-02-14 03:15:50+00:00</td>\n",
       "      <td>b'Went back to the Dr today. Sinus migraine ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>287</td>\n",
       "      <td>728995464494845953</td>\n",
       "      <td>2023-02-14 03:15:45+00:00</td>\n",
       "      <td>b\"RT @Club_anti_LFI: Dr Sarah Godfarb @PeterPa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>288</td>\n",
       "      <td>833966882604478464</td>\n",
       "      <td>2023-02-14 03:15:42+00:00</td>\n",
       "      <td>b'@Sinus_green \\xe3\\x82\\x88\\xe3\\x82\\x8d\\xe3\\x8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4199 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                   id                       Time  \\\n",
       "0              0  1590057131499307008  2023-02-14 10:27:56+00:00   \n",
       "1              1  1396131266207100931  2023-02-14 10:27:43+00:00   \n",
       "2              2  1431251001198067724  2023-02-14 10:23:57+00:00   \n",
       "3              3  1424407901159006215  2023-02-14 10:20:45+00:00   \n",
       "4              4  1571175125999030272  2023-02-14 10:20:18+00:00   \n",
       "...          ...                  ...                        ...   \n",
       "4194         284  1196038635658723329  2023-02-14 03:16:29+00:00   \n",
       "4195         285            491441655  2023-02-14 03:16:13+00:00   \n",
       "4196         286            191027326  2023-02-14 03:15:50+00:00   \n",
       "4197         287   728995464494845953  2023-02-14 03:15:45+00:00   \n",
       "4198         288   833966882604478464  2023-02-14 03:15:42+00:00   \n",
       "\n",
       "                                                  Tweet  \n",
       "0                                 b'@stqrkkura anemia.'  \n",
       "1     b'@GRozzana Chama creo que lo mio puede ser an...  \n",
       "2     b'@jenhoonieee Heheheehehehehehheheeheeee ,,,,...  \n",
       "3     b'A 51 year old gentleman presented with sever...  \n",
       "4     b\"@tuffimitten @agenetics1 Don't remember the ...  \n",
       "...                                                 ...  \n",
       "4194  b'RT @TorukoInNihon: \\xe6\\x97\\xa5\\xe6\\x9c\\xac\\...  \n",
       "4195  b\"@jsalamipeter @Your_Miss_B No sinus issues b...  \n",
       "4196  b'Went back to the Dr today. Sinus migraine ba...  \n",
       "4197  b\"RT @Club_anti_LFI: Dr Sarah Godfarb @PeterPa...  \n",
       "4198  b'@Sinus_green \\xe3\\x82\\x88\\xe3\\x82\\x8d\\xe3\\x8...  \n",
       "\n",
       "[4199 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12bacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'Time', 'Tweet'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98090462",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df=tweets_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729d5898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1590057131499307008</td>\n",
       "      <td>2023-02-14 10:27:56+00:00</td>\n",
       "      <td>b'@stqrkkura anemia.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1396131266207100931</td>\n",
       "      <td>2023-02-14 10:27:43+00:00</td>\n",
       "      <td>b'@GRozzana Chama creo que lo mio puede ser an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1431251001198067724</td>\n",
       "      <td>2023-02-14 10:23:57+00:00</td>\n",
       "      <td>b'@jenhoonieee Heheheehehehehehheheeheeee ,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1424407901159006215</td>\n",
       "      <td>2023-02-14 10:20:45+00:00</td>\n",
       "      <td>b'A 51 year old gentleman presented with sever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1571175125999030272</td>\n",
       "      <td>2023-02-14 10:20:18+00:00</td>\n",
       "      <td>b\"@tuffimitten @agenetics1 Don't remember the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                       Time  \\\n",
       "0  1590057131499307008  2023-02-14 10:27:56+00:00   \n",
       "1  1396131266207100931  2023-02-14 10:27:43+00:00   \n",
       "2  1431251001198067724  2023-02-14 10:23:57+00:00   \n",
       "3  1424407901159006215  2023-02-14 10:20:45+00:00   \n",
       "4  1571175125999030272  2023-02-14 10:20:18+00:00   \n",
       "\n",
       "                                               Tweet  \n",
       "0                              b'@stqrkkura anemia.'  \n",
       "1  b'@GRozzana Chama creo que lo mio puede ser an...  \n",
       "2  b'@jenhoonieee Heheheehehehehehheheeheeee ,,,,...  \n",
       "3  b'A 51 year old gentleman presented with sever...  \n",
       "4  b\"@tuffimitten @agenetics1 Don't remember the ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605709be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove Punctuation like link, emoji, etc\n",
    "def preprocessTweets(tweet):\n",
    "  tweet=tweet.lower()\n",
    "  #Remove Urls\n",
    "  tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet)\n",
    "  #remove @reference and # from tweet\n",
    "  tweet = re.sub(r'\\@\\w+|\\#|\\d+','',tweet)\n",
    "  #remove stopword\n",
    "  tweet_token =word_tokenize(tweet) #Convert string to token\n",
    "  filtered_words =  [w for w in tweet_token if w not in stop_words]\n",
    "  #remove Punctuation\n",
    "  unpunctuated_words = [char for char in filtered_words if char not in string.punctuation]\n",
    "  unpunctuated_words = ' '.join(unpunctuated_words)\n",
    "\n",
    "  \n",
    "\n",
    "  return \" \".join(unpunctuated_words)\n",
    "\n",
    "# Function to obtain adjectives from tweets\n",
    "def getAdjective(tweet):\n",
    "  tweet = word_tokenize(tweet)\n",
    "  tweet = [word for(word, tag) in pos_tag(tweet) if tag ==\"JJ\" ]\n",
    "  return \" \".joinoin(tweet)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8765a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m alphabets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(string\u001b[38;5;241m.\u001b[39mascii_lowercase)\n\u001b[0;32m      9\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m stop_words \u001b[38;5;241m+\u001b[39m user_stop_words \u001b[38;5;241m+\u001b[39m alphabets   \n\u001b[1;32m---> 10\u001b[0m word_list \u001b[38;5;241m=\u001b[39m \u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "user_stop_words = ['2020', 'year', 'many', 'much', 'amp', 'next', 'cant', 'wont', 'hadnt',\n",
    "                    'havent', 'hasnt', 'isnt', 'shouldnt', 'couldnt', 'wasnt', 'werent',\n",
    "                    'mustnt', 'â€™', '...', '..', '.', '.....', '....', 'beenâ€¦', 'one', 'two',\n",
    "                    'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'aht',\n",
    "                    've', 'next']\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_word = stop_words + user_stop_words + alphabets   \n",
    "word_list = words.words()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return words to their base form using Lemmatizer\n",
    "def preprocessTweetsSentiments(tweet):\n",
    "  tweet_tokens= word_tokenize(tweet)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "  return \" \".join(lemma_words)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Processed_Tweets']= tweets_df['Tweet'].apply(preprocessTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets_Adjectives']= tweets_df['Processed_Tweets'].apply(getAdjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return words to their base form using Lemmatizer\n",
    "def preprocessTweetsSentiments(tweet):\n",
    "  tweet_tokens= word_tokenize(tweet)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "  return \" \".join(lemma_words)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets_Sentiment'] = tweets_df['Processed_Tweets'].apply(preprocessTweetsSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533070a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long_string = tweets_df['Tweets_Adjectives'].tolist()\n",
    "tweets_list=[]\n",
    "for item in tweets_long_string:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        tweets_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8aa40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Built-in Python Collections module to determine Word frequency\n",
    "from collections import Counter\n",
    "counts = Counter(tweets_list)\n",
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df.columns = ['Words', 'Count']\n",
    "df.sort_values(by='Count', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ad366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing tweet and reTweet symbol/text\n",
    "for i in range(0,len(tweets_df['Tweets_Sentiment'])):\n",
    "    s=tweets_df['Tweets_Sentiment'][i]\n",
    "    s=s.replace(\"b'rt\",\"\")\n",
    "    s=s.replace(\"b'\",\"\")\n",
    "    tweets_df['Tweets_Sentiment'][i]=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Subjectivity score\n",
    "def getSubjectivity(tweet):\n",
    "  return TextBlob(tweet).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(tweet):\n",
    "  return TextBlob(tweet).sentiment.polarity\n",
    "def getSentimentTextBlob(polarity):\n",
    "  if polarity<0:\n",
    "    return \"Negative\"\n",
    "  elif polarity ==0:\n",
    "    return \"Neutral\"\n",
    "  else:\n",
    "    return \"Positive\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Subjectivity']= tweets_df['Tweets_Sentiment'].apply(getSubjectivity)\n",
    "tweets_df['Polarity'] = tweets_df['Tweets_Sentiment'].apply(getPolarity)\n",
    "tweets_df['Sentiment'] = tweets_df['Polarity'].apply(getSentimentTextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ef8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=\"Tweet\",keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23d426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
