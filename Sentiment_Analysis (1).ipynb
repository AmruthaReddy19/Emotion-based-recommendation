{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae86fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ea853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag \n",
    "from textblob import TextBlob \n",
    "from wordcloud import WordCloud\n",
    "import numpy as np \n",
    "import re \n",
    "import string \n",
    "import glob \n",
    "from nltk import pos_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39eb2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2573dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('Final_Dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bcdac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.578530e+18</td>\n",
       "      <td>2023-02-13 17:09:44+00:00</td>\n",
       "      <td>b'runny nose tear eyes fever headache loss of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.904996e+07</td>\n",
       "      <td>2023-02-13 15:34:08+00:00</td>\n",
       "      <td>b'@katkinneywrites i have hot/cold sometimes. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9.234178e+07</td>\n",
       "      <td>2023-02-13 13:57:31+00:00</td>\n",
       "      <td>b'cytomegalovirus colitis - complication when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.179430e+18</td>\n",
       "      <td>2023-02-13 11:38:54+00:00</td>\n",
       "      <td>b'@fatalkci l know otli and matli for nausea a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8.566819e+08</td>\n",
       "      <td>2023-02-13 01:57:45+00:00</td>\n",
       "      <td>b'went from a fever to a cough to a sore throa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7475</th>\n",
       "      <td>7475</td>\n",
       "      <td>7475</td>\n",
       "      <td>7.051626e+08</td>\n",
       "      <td>2023-02-05 11:45:11+00:00</td>\n",
       "      <td>b'#nowplaying jump (mega banton) by dengue fev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7476</th>\n",
       "      <td>7476</td>\n",
       "      <td>7476</td>\n",
       "      <td>6.354304e+07</td>\n",
       "      <td>2023-02-05 11:35:22+00:00</td>\n",
       "      <td>b\"rt @chris_c_chapman: blood counts still very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>7477</td>\n",
       "      <td>7477</td>\n",
       "      <td>1.273950e+18</td>\n",
       "      <td>2023-02-05 11:30:20+00:00</td>\n",
       "      <td>b'rt @shukladeepak73: different activities by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7478</th>\n",
       "      <td>7478</td>\n",
       "      <td>7478</td>\n",
       "      <td>3.306546e+09</td>\n",
       "      <td>2023-02-05 11:26:45+00:00</td>\n",
       "      <td>b'dengue: 1 more dies, 5 patients hospitalised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7479</th>\n",
       "      <td>7479</td>\n",
       "      <td>7479</td>\n",
       "      <td>1.282360e+18</td>\n",
       "      <td>2023-02-05 11:14:11+00:00</td>\n",
       "      <td>b\"if only your name rhymed with dengue shock s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7480 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1            id                       Time  \\\n",
       "0              0             0  1.578530e+18  2023-02-13 17:09:44+00:00   \n",
       "1              1             1  4.904996e+07  2023-02-13 15:34:08+00:00   \n",
       "2              2             2  9.234178e+07  2023-02-13 13:57:31+00:00   \n",
       "3              3             3  1.179430e+18  2023-02-13 11:38:54+00:00   \n",
       "4              4             4  8.566819e+08  2023-02-13 01:57:45+00:00   \n",
       "...          ...           ...           ...                        ...   \n",
       "7475        7475          7475  7.051626e+08  2023-02-05 11:45:11+00:00   \n",
       "7476        7476          7476  6.354304e+07  2023-02-05 11:35:22+00:00   \n",
       "7477        7477          7477  1.273950e+18  2023-02-05 11:30:20+00:00   \n",
       "7478        7478          7478  3.306546e+09  2023-02-05 11:26:45+00:00   \n",
       "7479        7479          7479  1.282360e+18  2023-02-05 11:14:11+00:00   \n",
       "\n",
       "                                                  Tweet  \n",
       "0     b'runny nose tear eyes fever headache loss of ...  \n",
       "1     b'@katkinneywrites i have hot/cold sometimes. ...  \n",
       "2     b'cytomegalovirus colitis - complication when ...  \n",
       "3     b'@fatalkci l know otli and matli for nausea a...  \n",
       "4     b'went from a fever to a cough to a sore throa...  \n",
       "...                                                 ...  \n",
       "7475  b'#nowplaying jump (mega banton) by dengue fev...  \n",
       "7476  b\"rt @chris_c_chapman: blood counts still very...  \n",
       "7477  b'rt @shukladeepak73: different activities by ...  \n",
       "7478  b'dengue: 1 more dies, 5 patients hospitalised...  \n",
       "7479  b\"if only your name rhymed with dengue shock s...  \n",
       "\n",
       "[7480 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12bacdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'Time', 'Tweet'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98090462",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df=tweets_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605709be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8765a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m alphabets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(string\u001b[38;5;241m.\u001b[39mascii_lowercase)\n\u001b[0;32m      9\u001b[0m stop_word \u001b[38;5;241m=\u001b[39m stop_words \u001b[38;5;241m+\u001b[39m user_stop_words \u001b[38;5;241m+\u001b[39m alphabets   \n\u001b[1;32m---> 10\u001b[0m word_list \u001b[38;5;241m=\u001b[39m \u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Asus/nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Asus\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "user_stop_words = ['2020', 'year', 'many', 'much', 'amp', 'next', 'cant', 'wont', 'hadnt',\n",
    "                    'havent', 'hasnt', 'isnt', 'shouldnt', 'couldnt', 'wasnt', 'werent',\n",
    "                    'mustnt', 'â€™', '...', '..', '.', '.....', '....', 'beenâ€¦', 'one', 'two',\n",
    "                    'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'aht',\n",
    "                    've', 'next']\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_word = stop_words + user_stop_words + alphabets   \n",
    "word_list = words.words()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return words to their base form using Lemmatizer\n",
    "def preprocessTweetsSentiments(tweet):\n",
    "  tweet_tokens= word_tokenize(tweet)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "  return \" \".join(lemma_words)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Processed_Tweets']= tweets_df['Tweet'].apply(preprocessTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets_Adjectives']= tweets_df['Processed_Tweets'].apply(getAdjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return words to their base form using Lemmatizer\n",
    "def preprocessTweetsSentiments(tweet):\n",
    "  tweet_tokens= word_tokenize(tweet)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_words = [lemmatizer.lemmatize(w) for w in tweet_tokens]\n",
    "  return \" \".join(lemma_words)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Tweets_Sentiment'] = tweets_df['Processed_Tweets'].apply(preprocessTweetsSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533070a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_long_string = tweets_df['Tweets_Adjectives'].tolist()\n",
    "tweets_list=[]\n",
    "for item in tweets_long_string:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        tweets_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8aa40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Built-in Python Collections module to determine Word frequency\n",
    "from collections import Counter\n",
    "counts = Counter(tweets_list)\n",
    "df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "df.columns = ['Words', 'Count']\n",
    "df.sort_values(by='Count', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ad366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing tweet and reTweet symbol/text\n",
    "for i in range(0,len(tweets_df['Tweets_Sentiment'])):\n",
    "    s=tweets_df['Tweets_Sentiment'][i]\n",
    "    s=s.replace(\"b'rt\",\"\")\n",
    "    s=s.replace(\"b'\",\"\")\n",
    "    tweets_df['Tweets_Sentiment'][i]=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Subjectivity score\n",
    "def getSubjectivity(tweet):\n",
    "  return TextBlob(tweet).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(tweet):\n",
    "  return TextBlob(tweet).sentiment.polarity\n",
    "def getSentimentTextBlob(polarity):\n",
    "  if polarity<0:\n",
    "    return \"Negative\"\n",
    "  elif polarity ==0:\n",
    "    return \"Neutral\"\n",
    "  else:\n",
    "    return \"Positive\"    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Subjectivity']= tweets_df['Tweets_Sentiment'].apply(getSubjectivity)\n",
    "tweets_df['Polarity'] = tweets_df['Tweets_Sentiment'].apply(getPolarity)\n",
    "tweets_df['Sentiment'] = tweets_df['Polarity'].apply(getSentimentTextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ef8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=\"Tweet\",keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23d426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
